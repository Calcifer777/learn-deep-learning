{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "06a554b5-b485-48f9-91d7-b632d31e3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import he_normal, glorot_uniform\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1cd3ca9-9d9a-4c3c-bd8e-08be8fe5a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), info = tfds.load(\"mnist\", split=[\"train\", \"test\"], with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e776aab-0ec7-424f-b496-2f18b47e3dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='/home/calcifer/tensorflow_datasets/mnist/3.0.1',\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487e5f84-4d57-4cef-bd45-d14c064e4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To numpy\n",
    "x_train = np.stack([record['image'] for record in ds_train.as_numpy_iterator()])\n",
    "y_train = np.stack([record['label'] for record in ds_train.as_numpy_iterator()])\n",
    "x_test = np.stack([record['image'] for record in ds_test.as_numpy_iterator()])\n",
    "y_test = np.stack([record['label'] for record in ds_test.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9833929-c86d-4492-9391-6c85ed1201e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X train: (60000, 28, 28, 1)\n",
      "Shape y train: (60000,)\n",
      "Shape X test: (10000, 28, 28, 1)\n",
      "Shape y test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape X train:\", x_train.shape)\n",
    "print(\"Shape y train:\", y_train.shape)\n",
    "print(\"Shape X test:\", x_test.shape)\n",
    "print(\"Shape y test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f487e21c-856e-481e-b8bf-694a9416770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "x_train_std = (x_train - 255) / 255\n",
    "x_test_std = (x_test - 255) / 255\n",
    "y_train_ohc = to_categorical(y_train)\n",
    "y_test_ohc = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7da842a0-4878-4f57-a1fd-8488672daa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some samples\n",
    "idx = np.random.randint(60000)\n",
    "fig = plt.figure\n",
    "plt.imshow(x_train[idx].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bdb83cd-2104-41ff-b5a4-26e02c46c5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "2D convolution layer (e.g. spatial convolution over images).\n",
       "\n",
       "This layer creates a convolution kernel that is convolved\n",
       "with the layer input to produce a tensor of\n",
       "outputs. If `use_bias` is True,\n",
       "a bias vector is created and added to the outputs. Finally, if\n",
       "`activation` is not `None`, it is applied to the outputs as well.\n",
       "\n",
       "When using this layer as the first layer in a model,\n",
       "provide the keyword argument `input_shape`\n",
       "(tuple of integers or `None`, does not include the sample axis),\n",
       "e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n",
       "in `data_format=\"channels_last\"`. You can use `None` when\n",
       "a dimension has variable size.\n",
       "\n",
       "Examples:\n",
       "\n",
       ">>> # The inputs are 28x28 RGB images with `channels_last` and the batch\n",
       ">>> # size is 4.\n",
       ">>> input_shape = (4, 28, 28, 3)\n",
       ">>> x = tf.random.normal(input_shape)\n",
       ">>> y = tf.keras.layers.Conv2D(\n",
       "... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
       ">>> print(y.shape)\n",
       "(4, 26, 26, 2)\n",
       "\n",
       ">>> # With `dilation_rate` as 2.\n",
       ">>> input_shape = (4, 28, 28, 3)\n",
       ">>> x = tf.random.normal(input_shape)\n",
       ">>> y = tf.keras.layers.Conv2D(\n",
       "... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\n",
       ">>> print(y.shape)\n",
       "(4, 24, 24, 2)\n",
       "\n",
       ">>> # With `padding` as \"same\".\n",
       ">>> input_shape = (4, 28, 28, 3)\n",
       ">>> x = tf.random.normal(input_shape)\n",
       ">>> y = tf.keras.layers.Conv2D(\n",
       "... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n",
       ">>> print(y.shape)\n",
       "(4, 28, 28, 2)\n",
       "\n",
       ">>> # With extended batch shape [4, 7]:\n",
       ">>> input_shape = (4, 7, 28, 28, 3)\n",
       ">>> x = tf.random.normal(input_shape)\n",
       ">>> y = tf.keras.layers.Conv2D(\n",
       "... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n",
       ">>> print(y.shape)\n",
       "(4, 7, 26, 26, 2)\n",
       "\n",
       "\n",
       "Args:\n",
       "  filters: Integer, the dimensionality of the output space (i.e. the number of\n",
       "    output filters in the convolution).\n",
       "  kernel_size: An integer or tuple/list of 2 integers, specifying the height\n",
       "    and width of the 2D convolution window. Can be a single integer to specify\n",
       "    the same value for all spatial dimensions.\n",
       "  strides: An integer or tuple/list of 2 integers, specifying the strides of\n",
       "    the convolution along the height and width. Can be a single integer to\n",
       "    specify the same value for all spatial dimensions. Specifying any stride\n",
       "    value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n",
       "  padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
       "    `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n",
       "    to the left/right or up/down of the input. When `padding=\"same\"` and\n",
       "    `strides=1`, the output has the same size as the input.\n",
       "  data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
       "    The ordering of the dimensions in the inputs. `channels_last` corresponds\n",
       "    to inputs with shape `(batch_size, height, width, channels)` while\n",
       "    `channels_first` corresponds to inputs with shape `(batch_size, channels,\n",
       "    height, width)`. It defaults to the `image_data_format` value found in\n",
       "    your Keras config file at `~/.keras/keras.json`. If you never set it, then\n",
       "    it will be `channels_last`.\n",
       "  dilation_rate: an integer or tuple/list of 2 integers, specifying the\n",
       "    dilation rate to use for dilated convolution. Can be a single integer to\n",
       "    specify the same value for all spatial dimensions. Currently, specifying\n",
       "    any `dilation_rate` value != 1 is incompatible with specifying any stride\n",
       "    value != 1.\n",
       "  groups: A positive integer specifying the number of groups in which the\n",
       "    input is split along the channel axis. Each group is convolved separately\n",
       "    with `filters / groups` filters. The output is the concatenation of all\n",
       "    the `groups` results along the channel axis. Input channels and `filters`\n",
       "    must both be divisible by `groups`.\n",
       "  activation: Activation function to use. If you don't specify anything, no\n",
       "    activation is applied (see `keras.activations`).\n",
       "  use_bias: Boolean, whether the layer uses a bias vector.\n",
       "  kernel_initializer: Initializer for the `kernel` weights matrix (see\n",
       "    `keras.initializers`). Defaults to 'glorot_uniform'.\n",
       "  bias_initializer: Initializer for the bias vector (see\n",
       "    `keras.initializers`). Defaults to 'zeros'.\n",
       "  kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
       "    matrix (see `keras.regularizers`).\n",
       "  bias_regularizer: Regularizer function applied to the bias vector (see\n",
       "    `keras.regularizers`).\n",
       "  activity_regularizer: Regularizer function applied to the output of the\n",
       "    layer (its \"activation\") (see `keras.regularizers`).\n",
       "  kernel_constraint: Constraint function applied to the kernel matrix (see\n",
       "    `keras.constraints`).\n",
       "  bias_constraint: Constraint function applied to the bias vector (see\n",
       "    `keras.constraints`).\n",
       "\n",
       "Input shape:\n",
       "  4+D tensor with shape: `batch_shape + (channels, rows, cols)` if\n",
       "    `data_format='channels_first'`\n",
       "  or 4+D tensor with shape: `batch_shape + (rows, cols, channels)` if\n",
       "    `data_format='channels_last'`.\n",
       "\n",
       "Output shape:\n",
       "  4+D tensor with shape: `batch_shape + (filters, new_rows, new_cols)` if\n",
       "  `data_format='channels_first'` or 4+D tensor with shape: `batch_shape +\n",
       "    (new_rows, new_cols, filters)` if `data_format='channels_last'`.  `rows`\n",
       "    and `cols` values might have changed due to padding.\n",
       "\n",
       "Returns:\n",
       "  A tensor of rank 4+ representing\n",
       "  `activation(conv2d(inputs, kernel) + bias)`.\n",
       "\n",
       "Raises:\n",
       "  ValueError: if `padding` is `\"causal\"`.\n",
       "  ValueError: when both `strides > 1` and `dilation_rate > 1`.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/git/marco/learn-deep-learning/.env/lib/python3.8/site-packages/keras/layers/convolutional.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Conv2DTranspose, Conv2D\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Conv2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83f37271-b87c-4139-a229-bf95f7eea903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Just your regular densely-connected NN layer.\n",
       "\n",
       "`Dense` implements the operation:\n",
       "`output = activation(dot(input, kernel) + bias)`\n",
       "where `activation` is the element-wise activation function\n",
       "passed as the `activation` argument, `kernel` is a weights matrix\n",
       "created by the layer, and `bias` is a bias vector created by the layer\n",
       "(only applicable if `use_bias` is `True`). These are all attributes of\n",
       "`Dense`.\n",
       "\n",
       "Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
       "computes the dot product between the `inputs` and the `kernel` along the\n",
       "last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
       "For example, if input has dimensions `(batch_size, d0, d1)`,\n",
       "then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
       "along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
       "(there are `batch_size * d0` such sub-tensors).\n",
       "The output in this case will have shape `(batch_size, d0, units)`.\n",
       "\n",
       "Besides, layer attributes cannot be modified after the layer has been called\n",
       "once (except the `trainable` attribute).\n",
       "When a popular kwarg `input_shape` is passed, then keras will create\n",
       "an input layer to insert before the current layer. This can be treated\n",
       "equivalent to explicitly defining an `InputLayer`.\n",
       "\n",
       "Example:\n",
       "\n",
       ">>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
       ">>> model = tf.keras.models.Sequential()\n",
       ">>> model.add(tf.keras.Input(shape=(16,)))\n",
       ">>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
       ">>> # Now the model will take as input arrays of shape (None, 16)\n",
       ">>> # and output arrays of shape (None, 32).\n",
       ">>> # Note that after the first layer, you don't need to specify\n",
       ">>> # the size of the input anymore:\n",
       ">>> model.add(tf.keras.layers.Dense(32))\n",
       ">>> model.output_shape\n",
       "(None, 32)\n",
       "\n",
       "Args:\n",
       "  units: Positive integer, dimensionality of the output space.\n",
       "  activation: Activation function to use.\n",
       "    If you don't specify anything, no activation is applied\n",
       "    (ie. \"linear\" activation: `a(x) = x`).\n",
       "  use_bias: Boolean, whether the layer uses a bias vector.\n",
       "  kernel_initializer: Initializer for the `kernel` weights matrix.\n",
       "  bias_initializer: Initializer for the bias vector.\n",
       "  kernel_regularizer: Regularizer function applied to\n",
       "    the `kernel` weights matrix.\n",
       "  bias_regularizer: Regularizer function applied to the bias vector.\n",
       "  activity_regularizer: Regularizer function applied to\n",
       "    the output of the layer (its \"activation\").\n",
       "  kernel_constraint: Constraint function applied to\n",
       "    the `kernel` weights matrix.\n",
       "  bias_constraint: Constraint function applied to the bias vector.\n",
       "\n",
       "Input shape:\n",
       "  N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
       "  The most common situation would be\n",
       "  a 2D input with shape `(batch_size, input_dim)`.\n",
       "\n",
       "Output shape:\n",
       "  N-D tensor with shape: `(batch_size, ..., units)`.\n",
       "  For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
       "  the output would have shape `(batch_size, units)`.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/git/marco/learn-deep-learning/.env/lib/python3.8/site-packages/keras/layers/core/dense.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Dense\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "38f6e5c0-7996-4e58-b9d2-4f9ba0ac02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        input_shape=(28, 28, 1),\n",
    "        filters=8,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer='zeros',\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        input_shape=(28, 28, 1),\n",
    "        filters=4,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer='zeros',\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "model.add(Flatten())\n",
    "model.add(\n",
    "    Dense(\n",
    "        units=10,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer='zeros',\n",
    "        activation=\"softmax\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dd156f5f-266d-4985-a87e-f326c694d1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Configures the model for training.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
       "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
       "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
       "                       tf.keras.metrics.FalseNegatives()])\n",
       "```\n",
       "\n",
       "Args:\n",
       "    optimizer: String (name of optimizer) or optimizer instance. See\n",
       "      `tf.keras.optimizers`.\n",
       "    loss: Loss function. Maybe be a string (name of loss function), or\n",
       "      a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
       "      function is any callable with the signature `loss = fn(y_true,\n",
       "      y_pred)`, where `y_true` are the ground truth values, and\n",
       "      `y_pred` are the model's predictions.\n",
       "      `y_true` should have shape\n",
       "      `(batch_size, d0, .. dN)` (except in the case of\n",
       "      sparse loss functions such as\n",
       "      sparse categorical crossentropy which expects integer arrays of shape\n",
       "      `(batch_size, d0, .. dN-1)`).\n",
       "      `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
       "      The loss function should return a float tensor.\n",
       "      If a custom `Loss` instance is\n",
       "      used and reduction is set to `None`, return value has shape\n",
       "      `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
       "      values; otherwise, it is a scalar. If the model has multiple outputs,\n",
       "      you can use a different loss on each output by passing a dictionary\n",
       "      or a list of losses. The loss value that will be minimized by the\n",
       "      model will then be the sum of all individual losses, unless\n",
       "      `loss_weights` is specified.\n",
       "    metrics: List of metrics to be evaluated by the model during training\n",
       "      and testing. Each of this can be a string (name of a built-in\n",
       "      function), function or a `tf.keras.metrics.Metric` instance. See\n",
       "      `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
       "      function is any callable with the signature `result = fn(y_true,\n",
       "      y_pred)`. To specify different metrics for different outputs of a\n",
       "      multi-output model, you could also pass a dictionary, such as\n",
       "      `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
       "      You can also pass a list to specify a metric or a list of metrics\n",
       "      for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
       "      or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
       "      strings 'accuracy' or 'acc', we convert this to one of\n",
       "      `tf.keras.metrics.BinaryAccuracy`,\n",
       "      `tf.keras.metrics.CategoricalAccuracy`,\n",
       "      `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
       "      function used and the model output shape. We do a similar\n",
       "      conversion for the strings 'crossentropy' and 'ce' as well.\n",
       "    loss_weights: Optional list or dictionary specifying scalar coefficients\n",
       "      (Python floats) to weight the loss contributions of different model\n",
       "      outputs. The loss value that will be minimized by the model will then\n",
       "      be the *weighted sum* of all individual losses, weighted by the\n",
       "      `loss_weights` coefficients.\n",
       "        If a list, it is expected to have a 1:1 mapping to the model's\n",
       "          outputs. If a dict, it is expected to map output names (strings)\n",
       "          to scalar coefficients.\n",
       "    weighted_metrics: List of metrics to be evaluated and weighted by\n",
       "      `sample_weight` or `class_weight` during training and testing.\n",
       "    run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
       "      logic will not be wrapped in a `tf.function`. Recommended to leave\n",
       "      this as `None` unless your `Model` cannot be run inside a\n",
       "      `tf.function`. `run_eagerly=True` is not supported when using\n",
       "      `tf.distribute.experimental.ParameterServerStrategy`.\n",
       "    steps_per_execution: Int. Defaults to 1. The number of batches to\n",
       "      run during each `tf.function` call. Running multiple batches\n",
       "      inside a single `tf.function` call can greatly improve performance\n",
       "      on TPUs or small models with a large Python overhead.\n",
       "      At most, one full epoch will be run each\n",
       "      execution. If a number larger than the size of the epoch is passed,\n",
       "      the execution will be truncated to the size of the epoch.\n",
       "      Note that if `steps_per_execution` is set to `N`,\n",
       "      `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
       "      will only be called every `N` batches\n",
       "      (i.e. before/after each `tf.function` execution).\n",
       "    **kwargs: Arguments supported for backwards compatibility only.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/git/marco/learn-deep-learning/.env/lib/python3.8/site-packages/keras/engine/training.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8fb263c9-bbc3-4b3b-b098-4b006cfe83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 8)         136       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 4)         292       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,278\n",
      "Trainable params: 8,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2cb5a919-a75c-41a9-b70b-0d6ef24f6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "30497ad3-44d4-42d1-88a8-8cc98ecd45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.8979e-04 - accuracy: 0.9998 - val_loss: 0.3956 - val_accuracy: 0.9681\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.7813e-04 - accuracy: 0.9998 - val_loss: 0.3957 - val_accuracy: 0.9678\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.7868e-04 - accuracy: 0.9998 - val_loss: 0.3964 - val_accuracy: 0.9673\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.8192e-04 - accuracy: 0.9998 - val_loss: 0.3970 - val_accuracy: 0.9683\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.5955e-04 - accuracy: 0.9998 - val_loss: 0.3991 - val_accuracy: 0.9680\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.9193e-04 - accuracy: 0.9998 - val_loss: 0.4009 - val_accuracy: 0.9677\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 8.6018e-04 - accuracy: 0.9998 - val_loss: 0.4003 - val_accuracy: 0.9679\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 3s 14ms/step - loss: 8.5877e-04 - accuracy: 0.9998 - val_loss: 0.4010 - val_accuracy: 0.9681\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 3s 15ms/step - loss: 8.4865e-04 - accuracy: 0.9998 - val_loss: 0.4015 - val_accuracy: 0.9679\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 3s 14ms/step - loss: 8.7305e-04 - accuracy: 0.9998 - val_loss: 0.4062 - val_accuracy: 0.9677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47780a2e20>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=x_train_std,\n",
    "    y=y_train_ohc,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test_std, y_test_ohc),\n",
    "    shuffle=True,\n",
    "    workers=8,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "40bb773f-2c57-4fed-b9a2-8afd9e50944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 96.77%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = model.history.history['accuracy']\n",
    "acc_val =  model.history.history['val_accuracy']\n",
    "plt.plot(acc)\n",
    "plt.plot(acc_val)\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.ylim((0.0,1.1))\n",
    "print(\"Validation accuracy: {:.2%}\".format(acc_val[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "fea0ddb5-834b-4c36-92ba-724678f12c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  1\n",
      "Actual:  1\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(10000)\n",
    "fig = plt.figure\n",
    "plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print(\"Predicted: \", np.argmax(model.predict(x_test_std[idx:idx+1])[0]))\n",
    "print(\"Actual: \", y_test[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
