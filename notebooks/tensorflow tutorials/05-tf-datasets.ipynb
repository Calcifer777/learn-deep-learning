{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73fb4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 11:09:57.050390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-03 11:09:57.050421: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bffaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 09:48:59.964520: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-03 09:48:59.964549: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-03 09:48:59.964572: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calcifer-Inspiron-7370): /proc/driver/nvidia/version does not exist\n",
      "2022-06-03 09:48:59.965668: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(mnist_ds_train, mnist_ds_test) , mnist_info = tfds.load(\"mnist\", split=[\"train\", \"test\"], with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2fd69",
   "metadata": {},
   "source": [
    "# Explore a TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d253204",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='/home/calcifer/tensorflow_datasets/mnist/3.0.1',\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ee9e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 10:52:45.827247: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example in mnist_ds_train.take(1):\n",
    "  print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c3e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b503e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n",
      "image shape: (28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 10:51:42.448653: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for r in tfds.as_numpy(mnist_ds_train.take(1)):\n",
    "  print(\"Label:\", r['label'])\n",
    "  print(\"image shape:\", r[\"image\"].shape)\n",
    "  plt.imshow(r['image'])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dad79b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 10:52:20.210862: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAIJCAYAAADanCVIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAArYElEQVR4nO3dfbzVY77/8feH2iVFbkKFMlREpBsOJZG73JxhZMiQM/FjhCNOYzhzDPIwB42bMSWZkJlf02h046ZIjbvh4GjnLjUhulFR0a1qKl3njxazvt/rWnt991pr77XX2q/n4zGPR9dnX9/vvprH1ertuz9dX3POCQAA1G87FHsBAACg+AgEAACAQAAAAAgEAABABAIAACACAQAAkNSgOpPNjH+jCI9zzoq9hnywr5HBSudci2IvIh/sbYRk+szmCQEAhC0s9gKA2kQgAAAABAIAAEAgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAqubLjQAAKBft27f3aiNGjIiM+/Tp480ZM2aMVxs0aJBX27RpU+6LKwKeEAAAAAIBAAAgEAAAABEIAACAaCoEANRTxx57rFc78cQTI2PnnDfnkksu8WrffvutV7vqqqsi482bN1d3ibWKJwQAAIBAAAAACAQAAEAEAgAAIJoKa12/fv282vjx473aFVdc4dV+//vf18iagOraaaedIuMHH3zQm9OkSROv1r9/f6+2bdu2wi0MyOC0007zavfff3/B7j9w4ECvNmfOnMj4vvvuK9j3qwk8IQAAAAQCAABAIAAAACIQAAAA0VRY6y688EKvFjoJa/fdd6+N5QBZmZlXGzVqVGR80UUXJbrXf//3f3u1d999N6d1AZmEGlqHDh3q1Zo1a1aj67j55psjY5oKAQBAnUcgAAAABAIAAEAPQY1r06ZNZNy3b19vTmVlpVf705/+VGNrAqqjY8eOXi1Jz8DatWu92ldffVWQNQFVmTBhglfr1q2bVwv1b8WFelw6d+6caB0NGpTWX7E8IQAAAAQCAABAIAAAACIQAAAAlXFTYegwlZAkTSX5+Pd///fIuKKiwpvz6aeferXFixfX2JqA6jjvvPNyum7RokVejX2NQrvsssu8Wu/evXO+X/zz+Pjjj/fmhJoWTzrpJK8Wbyo88MADvTnz58+v7hJrDE8IAAAAgQAAABAIAACACAQAAEBl3FQYaioJvWnqZz/7WWT85ptvFnQdnTp1yjqHt72hLrv22muzztm6datXC73ZEMjXgAEDIuPhw4d7cxo2bJjoXp988olXO/XUUyPj9evXe3OSnrjZqFGjyDj09xJNhQAAoE4hEAAAAAIBAAAgEAAAAJVxU+HGjRu9WqjBL34KVT5Nhfvuu2/W+69bt86b8/jjj+f8PYFCat68uVfbdddds163YsUKrzZu3LhCLAn1WOvWrb3aTTfdFBknbSBctmyZV7viiiu82oIFC5ItLgd9+vTxao888kiNfb/q4gkBAAAgEAAAAAIBAAAQgQAAAKiMmwqXL19e69/znHPO8WrxhpeZM2d6c0LNLkAxDB06NKfrPvjggwKvBPVNqCl76tSpXq19+/Y53f/uu+/2ai+//HJO98rVoYceWqvfr7p4QgAAAAgEAACAQAAAAFTGPQS77757rX/PVq1aZZ1T2z+zAqrjsssuy+m63/72twVeCeqb0AE9uf7MPfQG2TFjxuR0r0KqC2uoCk8IAAAAgQAAABAIAACACAQAAEBl3FQYOiTIzAp2/9BbuK688sqs3/PRRx8t2BqAYlm9enVkPH369OIsBCXp1FNP9Wonn3xyTvf65ptvvNrZZ5/t1dasWZPT/UNCf5ck+fsl9LbbuoQnBAAAgEAAAAAIBAAAQAQCAACgMmkqbNSokVe7/PLLvZpzzqv1798/Mm7btq03J3Tq4eGHH+7VmjVr5tXeeeedyPizzz7z5gDF0LlzZ68WfztnJiNGjIiMt27dWogloQw1b97cq40ePdqrhT6fQ+JNhJdccok3Z/HixckWl0BFRYVX22uvvbxaaP3ffvttZLxkyZKCrasm8IQAAAAQCAAAAIEAAACIQAAAAFQmTYUXXnihV0v6+uNOnTpFxqFmwaTNLiF33nlnZLxt27ac7wUU0t133+3VGjTwPxK2bNni1eJNhUAmoabvJK+Kz+SZZ56JjCdNmpTzvZK45pprvFrv3r0TXbtp06bI+LnnnivEkmoMTwgAAACBAAAAEAgAAIAIBAAAQGXSVNi9e3evtmHDBq8WevXw0qVLI+Ovv/7am7Ny5Uqv9uSTTyZa2/PPP59oHlCT2rRp49WOOeYYrxZqoP3kk0+82hdffFGYhaHs9OrVKzJ++umnc75XaD9OnTo15/vl4swzz8z52vgph926dfPmzJw5M+f7FxpPCAAAAIEAAAAQCAAAgMqkh2DQoEGJarnq16+fVzMzrzZx4kSvtnbt2oKtA8jVkCFDvNrOO++c6NrQAUZAJsOHD4+MQ2+BTerTTz/1amPHjs35fkmccMIJkXGPHj1yvlf8ILpVq1blfK/awBMCAABAIAAAAAQCAAAgAgEAAFCZNBXWtNDbFEMHZrz99tu1sRyg2pK+nS1kzJgxBVsHyt/48eMj49tuuy3nez3xxBP5LqdKF110kVe79dZbI+Mdd9wx5/vfcsstkfH8+fNzvldt4AkBAAAgEAAAAAIBAAAQgQAAAIimwkSOP/54rxZqKnzllVdqYzlAVkcccURk3L59+0TXTZ48uQZWg/qkkG/CjL8tUJIuvfTSyLhr167enMWLF3u1UGNt/M2Mmb5nXPwEQslvppSke+65J+u96hKeEAAAAAIBAAAgEAAAABEIAACAaCr0dOnSxas1aOD/3/TCCy94tTfffLNG1gRUV/wVtA0bNkx03dChQ2tiOUBOQq/tztUOO/j//RtqDoz78ssvvdq9997r1X7zm9/ktrA6hCcEAACAQAAAAAgEAABAkoUO2Mk42Sz55BI1ffp0r9anTx+vtmXLFq82ePBgrzZy5MiCrKsuc85ZsdeQj1Lf102bNvVq8+bNi4xbtmzpzVm1apVXC83bvHlzHqsraZXOuW7FXkQ+irG3W7VqFRlPmzbNm9OxY8faWs73zPyPqRUrVni1hx9+ODJ+5JFHvDkLFiwo2LqKIdNnNk8IAAAAgQAAABAIAACACAQAAEAcTOQJNVmGah9++KFXe/LJJ2tkTUBVQm8yDDUHxv3P//yPV6vHDYQokKVLl0bGoTcKXnDBBV7t5ptv9mp77713TmsYM2aMV3v22We92htvvOHVCvm2xlLDEwIAAEAgAAAABAIAACACAQAAEE2FnkMOOcSrffPNN17tRz/6kVcLnXoF1LSzzjorp+tGjx5d4JUAvtCJmKETXOvDqa51HU8IAAAAgQAAABAIAACACAQAAEC8/tizcuVKrxZqimnXrl1tLKck8Prj4tpzzz29WvwkzdCf8wMPPNCrhRpo6zFef4yyxOuPAQBARgQCAABAIAAAAAQCAAAgmgpRADQVokzRVIiyRFMhAADIiEAAAAAIBAAAgEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAAJIaVHP+SkkLa2IhKFltir2AAmBfI4S9jXKUcV9X6/XHAACgPPEjAwAAQCAAAAAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEHjMbEcze8fMnq1izv1m1itWe8DM1qeNrzazgTW5ViAJM3vUzJab2ews8wab2YDUr88zsw/NbJuZdUub08nMxtTwkoFEzOw0M5tnZp+Y2Y1VzPv+M9vMDjCzt1LXPGFmFal6vf/MJhD4rpU0N9MXzWwPSf/inHs1rdZN0m6xqY9KuqZGVghUzxhJp1U1wcwaSBoo6U+p0mxJP5L0avo859wHkvY1s/0Lv0wgOTPbUdIISX0ldZTU38w6BubFP7PvknSfc+4gSaskXZqq1/vPbAJBGjPbV9IZkkZXMe1cSc+nXbOjpGGSbkif5JzbIGmBmR1VA0sFEkt9EH6dZdqJkmY557amrpnrnJuXYe4zki4o4BKBXBwl6RPn3KfOuc2S/izph4F5339mm5lp+15/MvW1xyWdLfGZLREI4u7X9r/Yt1Uxp4ekyrTx1ZKeds4tC8ydKem4gq0OqDnxfV0V9jXqgtaSFqeNP0/V4tL39h6SVn8XfAPX1Ou9TSBIMbMzJS13zmX7UGwpaUXqmlaSzpP0uwxzl0tqVbBFAjXn+32dAPsapYS9nRCB4J96SPpXM1ug7Y+eTjSz/x+Yt1FS49Svj5R0kKRPUtc1MbNP0uY2Ts0H6rr0fZ0N+xp1wRJJ+6WN903V4tL39leSmqd6ZkLX1Ou9TSBIcc7d5Jzb1znXVtt/Pvqic+6iwNS52h4C5Jyb4pzbxznXNnXdhlSjynfaa3tzFlDXfb+vE2Bfoy54W1K71L8aqND2z+2nA/PSP7OdpJck9Ut97RJJT6XNrdd7m0BQfVMk9U44t4ek6TW3FCA7Mxsn6Q1JHczsczO7NDDtOUm90q45x8w+l3SMpClmNi1t7gna/ucAKJpUH8DVkqZp+1/6451zHwamxj+zfyHp+tTT3D0kPZL2tXr9mW3bAxOqw8xek3Smc251FXOOlHS9c+7iWlsYkAczmyTpBufcx1XMaSTpFUk90xqzgDqNz+xkCAQ5MLOjJW10zr1fxZyTJX3snFtQawsD8mBmHSTtnX7GRmBOO0mtnXMv19rCgDzxmZ0MgQAAANBDAAAACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACApAbVmWxmrqYWgtLlnLNiryEf7GtksNI516LYi8gHexshmT6zeUIAAGELi70AoDYRCAAAAIEAAAAQCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAACQ1KPYCAJS+Zs2aebWrrrrKq/3617/2asuWLYuMO3bs6M1Zs2ZNHqsDwho1auTVXn/99cj4Bz/4gTfnpJNO8mqzZs0q3MKKhCcEAACAQAAAAAgEAABABAIAACDJnHPJJ5sln4x6wzlnxV5DPspxX8cboUJNf+eee65Xa9y4cdZ7hWrvvfeeN2fAgAFZ1ylJZtHt07JlS2/Ol19+meheBVbpnOtWjG9cKOW4twtpn3328WpLly7Net3s2bO9Wvfu3b3aP/7xj9wWVsMyfWbzhAAAABAIAAAAgQAAAIhAAAAAVMYnFb700kterXfv3l7trrvuioxvvPHGmloSUC2hU9QOOOAArzZy5EivduSRR0bGu+yyizenOg3FcfFGwCOOOCLnewHFcuutt+Z0XejPU4sWLbza559/ntP9i4UnBAAAgEAAAAAIBAAAQCXYQxD/2aUkdejQwavFf4YqSdu2bfNq1157bWT87bffenMmTpzo1UI/f503b55XizvxxBO9WujglwULFni1qVOnRsZbtmzJ+v1QGkJ7YPz48V4ttK+TiL/BTZLmz5/v1aZMmeLVVq9e7dWmTZuW0zpClixZEhlv2rSpYPcGvnPOOed4tSuuuMKrJemtmTNnjlcrtX6BEJ4QAAAAAgEAACAQAAAAEQgAAIBKsKmwU6dOXu2dd97J+X4VFRWRcehgorpyWNHf/va3yDjUJLNq1araWg7y0Ldv38g41MwXsm7dOq8WOoRr2LBhkXGoqTCpiy++OOuc9evXJ7pX6K2Lf/3rXyPjNWvWJFsYUA0HH3xwTtfFm14laeDAgfkup07iCQEAACAQAAAAAgEAABCBAAAASLLqvPHMzHJ/PVqO2rRpExmHGqjiczJZu3atV4ufXrjbbrt5c5L+fxQ6RTHJtaEmql133TXr/R966CFvzqBBg7J+v0Jzzvm/8RJS0/v60EMP9WqzZs2KjBs08Pt7//d//9er9evXz6uFmp4KqWPHjl7tyiuvjIxDp7Rdd911Xi30RrimTZtGxhs3bqzuEmtKpXOuW7EXkY9ifGbXVXPnzvVqoUbD+Gf2Lbfc4s25/fbbC7ewIsj0mc0TAgAAQCAAAAAEAgAAIAIBAABQCZxUePnll0fGSRsI77rrLq92//33e7V4A1Po9cQ1bfbs2V7to48+ynpd6NQ31D2HH364Vws1EcadfvrpXq0YJ1GGXvV6zTXXRMb9+/f35oQaCDds2ODV6lATIcpEaD+2a9cup3stXrw43+WUDJ4QAAAAAgEAACAQAAAAEQgAAIDqWFNhz549vdrgwYNzutcDDzzg1ZYvX571uqeeeiqn75ePgw46KNG8+Alap556qjencePGXm3Tpk25LQwFceSRR+Z0XdeuXb3ajBkz8l1Ojfj5z3+eaN4999xTwysBpJtvvtmr7bBDsv/+XbFiRWQ8ceLEgqypFPCEAAAAEAgAAACBAAAAqI71EIR+xh//mfjmzZu9OcOHD/dqxTjAJVcXXnhhonnxtx1OmzbNm0O/QN0zduxYrzZkyJCs173wwguJ7v/ss896tfj+X7ZsmTdn8uTJXu3NN99M9D0vueSSyLhz587enC+++MKr3XrrrYnuD+Qj9NbapO67777IOPSW3HLFEwIAAEAgAAAABAIAACACAQAAUB1rKvz444+92qGHHhoZr1u3zpuzZMmSGltTbdhll10SzYsfTITSEHpb4BlnnBEZ33HHHd6c0L444IADst4rJN6QKknXXXedV/vqq6+y3kuSdt1118g4tDcXLVrk1Y444giv9t577yX6nkDIxRdf7NX22muvRNeuX7/eq9Xnw7N4QgAAAAgEAACAQAAAAEQgAAAAkqw6jWpmRldbnoYOHerVQm+Kq6io8Grxhsqzzz7bm/Pyyy/nvLZcOef8jrUSUlf3dbNmzbxa0qbC5s2bR8ahpsLQn/34CYSS1KJFC68Wv18+Da8ffPBBZBz68zB9+vSc75+HSudct2J840Kpq3u7kP7whz94tVCjYcjq1au9Wj6nHJaKTJ/ZPCEAAAAEAgAAQCAAAAAiEAAAANWxkwrL0e233x4Z33TTTd6cUMNXyOjRoyPjYjQQovaETuV8//33E9WSOOmkk7zaFVdckejaysrKyHjYsGHenNNPP92r9enTx6sdfvjhkfFf/vIXb06XLl282qeffpp1nSg/8Vdtn3XWWd6cpE2ud999dyGWVDZ4QgAAAAgEAACAQAAAAEQgAAAAoqkwZ6FGwJ/85Cde7T/+4z+yXhfy4osverUbb7wx4eqAqFtvvdWrhU4E3Gmnnbza66+/7tXiJxqGGvzGjx/v1Xr27OnVXn311cg49Nrnpk2bejXUT+3atYuM46/iro4pU6bku5yywhMCAABAIAAAAAQCAAAgeggSadu2rVe77bbbvFroDVtJDsiYN2+eV/vpT3/q1bZu3Zr1Xqh/GjZs6NUmT54cGfft29ebE9qbY8eO9WpXX321V1uzZk01VvhPoQOG4mbPnu3V5syZk9P3A6rSo0cPr5brQV/lgCcEAACAQAAAAAgEAABABAIAACDJkr4VSpLMLPnkEnXYYYd5tbvuusurnXbaaTndf9KkSV5tyJAhXm3BggU53b8YnHPJTluqo+rqvt5nn328Wr9+/bza+eefn/Xafffd15sT2teh2saNG6tcZyY777yzV5s5c6ZX69ChQ2QcOuBr3LhxOa0hT5XOuW7F+MaFUlf3dj4mTJgQGZ9zzjk53+ubb77xas2aNcv5fqUi02c2TwgAAACBAAAAEAgAAIAIBAAAQJxUqNatW0fGjzzyiDenW7fc+4rip7yNHDky53uhfMTfKvjggw96c+JvFJSSnXwpSTNmzIiMb7rpJm/Ok08+meheuerUqZNXa9++vVdbsmRJZDx16tQaWxNK3w9+8INiL6Fs8YQAAAAQCAAAAIEAAACIQAAAAERToa699trIuHv37t6cUCPX+vXrvdqNN97o1UaPHp3H6lAOjj76aK82fPjwyLhr167eHDP/MLF7773Xq91xxx1ebdWqVdVZYt72339/rzZlyhSvFvo93X777ZFxrq9WBqordHJsfcYTAgAAQCAAAAAEAgAAoHrWQxD/WaXk9xCE+gVCP9MMHfQyatSoPFaHcnXuued6tS5dukTGSQ8cmjt3rlcLvZ0t9DP9Qjr22GMj49Cfh+bNm3u1+fPne7WHH364YOtCeTn++OO92iGHHJLTvd5//32vNmDAgJzuVa54QgAAAAgEAACAQAAAAEQgAAAAKuOmwlBD04UXXujVGjSI/l8QOjjlz3/+s1ejgRBJjRkzxqudddZZkXHoLYAhoQa80CFEu+22W2Qc2tdJGxlD4vfbvHmzNyf01sLQn0EgkyZNmni1ioqKnO4VOigLUTwhAAAABAIAAEAgAAAAIhAAAACVcVNh//79vVrbtm2zXvfpp596tV//+teFWBLqqTlz5ni1zp07R8a9evXy5vTo0cOrhfbwTjvt5NX69euXfIFpQmutrKz0al988UVkPHnyZG/Om2++mdMagO9Mnz7dqw0ePDgyPvnkk705oRMxX3nllYKtq1zxhAAAABAIAAAAgQAAAIhAAAAAJFl1Tiszs9yPNqtlffv29Wqhk6riv/8rr7zSm8PrWavmnPOPwSshpbSvUasqnXPdir2IfLC3EZLpM5snBAAAgEAAAAAIBAAAQAQCAACgMj6p8MUXX/Rqb731llfr0KFD1usAACh3PCEAAAAEAgAAQCAAAAAq44OJUHs4mAhlioOJUJY4mAgAAGREIAAAAAQCAABAIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAAFD133a4UtLCmlgISlabYi+gANjXCGFvoxxl3NfVOroYAACUJ35kAAAACAQAAIBAAAAARCAAAAAiEAAAABEIAACACAQAAEAEgu+ZWQczezftf2vNbHCGuYPNbEDq10+kXbPAzN5N1TuZ2Zha+w0AGZjZdWb2oZnNNrNxZtY4w7z7zaxX6td9zGxWal+/ZmYHpepXm9nA2lw/kImZPWpmy81sdpZ56Z/Z56X+PGwzs25pc+r9ZzYHEwWY2Y6Slkg62jm3MPa1BpJmSerinNsa+9o9ktY454amxjMkDXTOLaqdlQNRZtZa0muSOjrnNprZeElTnXNjYvP2kDTFOfcvqfFHkn7onJtrZoMkHeWc+zczayLpdefckbX7OwF8qQC7XtIfnHOHZZgT+cw2s0MkbZM0StIQ59zMtLn1+jObJwRhfSTNj4eBlBMlzQqEAZP0Y0nj0srPSLqgxlYJJNNA0k6pD8YmkpYG5pwr6fm0sZO0S+rXu353jXNug6QFZnZUzS0XSMY596qkr7NMi3xmO+fmOufmZZhbrz+zCQRhFyj6F3u6HpIqA/XjJH3pnPs4rTYzVQeKwjm3RNJvJC2StEzbn2C9EJga39eXSZpqZp9LuljSnWlfY1+jlGT6zA6p13ubQBBjZhWS/lXSXzJMaSlpRaDeX36IWC6pVeFWB1SPme0m6YeSDtD2vbizmV0UmBrf19dJOt05t6+kxyTdm/Y19jVKSabP7JB6vbcJBL6+2v546csMX98oKdKUlXoU+yNJT8TmNk7NB4rlJEmfOedWOOe2SJoo6djAvO/3tZm1kHSEc+6t1NeeiF3DvkYp8T6zq1Cv9zaBwBf6L/10cyUdFKudJOnvzrnPY/X2kqrsfgVq2CJJ/2JmTVJ9Ln20fQ/Hpe/rVZJ2NbP2qfHJsWvY1ygloc/sTOr13iYQpDGznbX9w29iFdOek9QrVsvUc3CCpCmFWR1Qfan/yn9S27usP9D2P/MPB6ZOkdQ7dc1WSf9P0gQze0/bewh+nja3h6TpNbdqIBkzGyfpDUkdzOxzM7s0MC3ymW1m56R6Y46RNMXMpqXNrdef2fyzwxyY2SRJN8QaCONzGkl6RVLP+L9IAOoiM3tN0pnOudVVzDlS0vXOuYtrbWFAnvjMToZAkAMz6yBp79Q/eck0p52k1s65l2ttYUAezOxoSRudc+9XMedkSR875xbU2sKAPPGZnQyBAAAA0EMAAAAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAICkBtWZbGauphaC0uWcs2KvIR/sa2Sw0jnXotiLyAd7GyGZPrN5QgAAYQuLvQCgNhEIAAAAgQAAABAIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAAJDUo9gIK4Xe/+51X69q1a6Jrn3/++ch44cKF3pwvvvjCq02bNi3h6gAA5eTggw/2au+++65Xe/vttyPj4447rqaWVBA8IQAAAAQCAABAIAAAACIQAAAAlUBTYaNGjSLjESNGeHMGDhyY8/2POeaYyNg5583Ztm2bV5s5c6ZX+9WvfuXVXnjhhZzXBgCoe3r27OnVdtxxR6922GGHRcYHHnigN2f+/PmFW1ieeEIAAAAIBAAAgEAAAABEIAAAACqBpsIbbrghMs6ngTAk1EQYt8MOfm466qijvFqo4bF///6RcagZEagrevXq5dUeeOABr9ahQ4fI+Prrr/fmjBw5snALA4qkb9++Xi3UQN6ggf/X6YYNGyLjTZs2FW5hNYAnBAAAgEAAAAAIBAAAQCXQQ9CqVauscyZOnOjV3nvvPa+2fv16r/bHP/4xMo4fhCRJY8eO9WrHHnusVwsdOvHwww9Hxt27d/fmfPvtt14NCGnatKlX27p1q1eL78X4ASlSeA+Hegg6deqUdV3xA74keghQmuIHDA0aNMibs99++3m10Of4X//618h4yZIlea6uZvGEAAAAEAgAAACBAAAAiEAAAABUAk2F8cakRYsWeXPuvvtur1bIRr3evXt7teeff96rnXLKKV6tc+fOkfHPfvYzb07oQCOUtyZNmkTGU6dOTXTd5s2bvdpBBx3k1fbee+/IuHHjxt4cM/NqSQ7qClm3bl1O1wF1zdChQyPjM888M9F1b7/9tlcbMGBAQdZUW3hCAAAACAQAAIBAAAAARCAAAACSrDpNRGaWW8dRGerZs6dXmzFjhlerqKiIjJcvX+7NCb05MdQ8WVc55/zutBJSjH29xx57RMahfZFP01/8rWqh0wwfe+yxrOuSpPPPP9+rxU9zC70R8brrrsu6zjqu0jnXrdiLyAef2VU7+OCDvVplZWVkHG8AlsJN62eddZZXe+655/JYXfV16+Zv19AbdjN9ZvOEAAAAEAgAAACBAAAAiEAAAABUAicV1lWvvfaaVxs2bJhX++UvfxkZ77XXXt6ctm3berVSaipE9cVP9jvjjDMKev8FCxZExmvXrvXmLF26NNG9Qk2v8dMRQ/cH6pJQc+Att9ySaF7cuHHjvFptNxCGbNiwIa/reUIAAAAIBAAAgEAAAABEIAAAAKKpsKCeeuoprxZvKgzp1KmTV3v11VcLsibUTfHXGIdep10MzZs392qhJqv4KYrxJkagrgmdJHjBBRdkve7rr7/2aqNGjSrImgptzpw5eV3PEwIAAEAgAAAABAIAACB6COqE0M+2HnroIa8WesMWUEgdOnTwaq1atfJq8bcunnDCCd6c0NsUgdrQu3dvr/b4448nuja+t6+//npvTuhgunLAEwIAAEAgAAAABAIAACACAQAAEE2FBbVixQqvtnLlysh4zz339ObE3xwnSRUVFV5t48aNeawOyC50SFYSH3zwQYFXAuTuV7/6lVdr1KhRomuHDx8eGSdtRiwHPCEAAAAEAgAAQCAAAAAiEAAAAJVgU2HobWyhk9RCtm7d6tU++uijfJf0vRYtWni1UBNh3H333efVaCBEMeTaVFjIP0dAdVx55ZVerWfPnomuXbhwoVf7r//6r7zXVKp4QgAAAAgEAACAQAAAAEQgAAAAKoGmwr59+0bGoQa89u3bJ7rX5s2bvdptt90WGU+dOtWb89577yW6/w9/+MNE8+I45Q35CO27eHPgZ5995s35yU9+4tUOPvjgnNYQP91Nkrp27erVQifIAdWx9957R8a/+MUvvDkNGzb0aqGm8mHDhnm1tWvX5rG60sYTAgAAQCAAAAAEAgAAIAIBAABQCTQVPvXUU5Fxgwa5Lzn0SuE77rgjMr7lllu8Oc8884xXmzJlile74YYbsq5hy5YtXu0f//hH1usASRo9erRXO//8873azjvvnPVeZubVnHOJ1hFv0A392QLyFfq8j7+OuE2bNonuFWreHjFiRG4LK1M8IQAAAAQCAABAIAAAAJIs6c8MJcnMkk8ukPiBKkl/XrRs2TKvFvoZ0imnnJLbwnI0d+5cr3booYfW6hokqUuXLl5tv/32i4zj/RuZOOf8H0aXkGLs61y1bdvWq40cOdKrHXjggZHxypUrvTmhHoL999/fq+2zzz5ebdq0aZFxqI9h3bp1Xq3EVDrnuhV7Efkopb0d0rlzZ6/2zjvvZL0udAjRj3/8Y682adKknNZV6jJ9ZvOEAAAAEAgAAACBAAAAiEAAAABUAgcTDR06NDIeNWqUNyd0eEVlZaVXu/zyy71a48aNI+O//e1v3pzWrVtnXWdS7dq182pLlizxanPmzPFqHTt2LNg6mjdv7tXiTWZNmjQp2PdDYSxYsMCrxd8IKknNmjWLjJM2+L344oteLdRUGH8rYhk0EKIOuvnmm3O67re//a1Xq68NhNXBEwIAAEAgAAAABAIAACACAQAAUAk0FT722GORcaip6ve//71XO/PMM73a0qVLvdobb7wRGe++++7VXGH1hBogW7ZsmaiWq0WLFnm1iRMnerV77rmnYN8TxZWkyS906mH37t0T3b9hw4bVXRJQpW7d/EMhQw2zSUyePDnP1dRPPCEAAAAEAgAAQCAAAAAiEAAAAJVAU2HcSy+95NWuv/56rzZs2DCvFmqiOuaYY7J+z82bN3u10Cs477jjDq/297//Pev9QwYOHOjVKioqIuPQaYxvv/22V1u9erVXC70OF/XLIYcc4tWSnk45YcKEQi8H9dyQIUO82k477ZT1uhkzZni1t956qyBrqm94QgAAAAgEAACAQAAAAFSCPQQhTz/9dKJa586dvdrhhx+e9f6vvvqqVwsdkFRI//mf/1mj9wdCPTXxN15msmzZsgKvBvXJXnvt5dWS9HOF3HnnnV5ty5YtOd2rvuMJAQAAIBAAAAACAQAAEIEAAACoTJoKk3r33XcT1YD6YM899/RqzrlE14YOCAOS2m233bza/vvvn9O9tm3blu9ykMITAgAAQCAAAAAEAgAAIAIBAABQPWsqBPBP7du3TzQvdCrn+++/X+DVoD757LPPvNqDDz7o1QYNGuTVvv7668h48eLFhVtYPccTAgAAQCAAAAAEAgAAIAIBAAAQTYUAsvjmm2+82qZNm4qwEpSLzZs3e7WrrroqUQ01hycEAACAQAAAAAgEAABABAIAACCaCgFkMWHChGIvAUAt4AkBAAAgEAAAAAIBAACQZM655JPNkk9GveGcs2KvIR/sa2RQ6ZzrVuxF5IO9jZBMn9k8IQAAAAQCAABAIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAqPpvO1wpaWFNLAQlq02xF1AA7GuEsLdRjjLu62odXQwAAMoTPzIAAAAEAgAAQCAAAAAiEAAAABEIAACACAQAAEAEgu+Z2X5m9pKZzTGzD83s2irmDjazAalfDzOzv5vZ+2Y2ycyap+qdzGxM7aweCDOzDmb2btr/1prZ4Axz0/f1E2nXLDCzd1N19jXqDDN71MyWm9nsLPPS9/Z5qc/4bWbWLW1Ovd/bnEOQYmYtJbV0zs0ys2aSKiWd7ZybE5vXQNIsSV2cc1vN7BRJL6Z+fZckOed+kZo7Q9JA59yiWv3NAAFmtqOkJZKOds4tjH0tsq9jX7tH0hrn3NDUmH2NOsHMeklaL+kPzrnDMsyJf2YfImmbpFGShjjnZqbNrdd7mycEKc65Zc65Walfr5M0V1LrwNQTJc367kPTOfdC2gfom5L2TZv7jKQLam7VQLX0kTQ/HgZSIvv6O2Zmkn4saVxamX2NOsE596qkr7NMi39mz3XOzcswt17vbQJBgJm1lXSkpLcCX+6h7U8PQgZKei5tPFPScQVdHJC7CxT9iz1dpn19nKQvnXMfp9XY1yglVX1mx9XrvU0giDGzppImSBrsnFsbmNJS0orAdb+UtFXS2LTyckmtamKdQHWYWYWkf5X0lwxTgvtaUn/5IYJ9jVKSaW+H1Ou9Xd2XG5U1M2uo7WFgrHNuYoZpGyU1jl33b5LOlNTHRZsyGqfmA8XWV9sfm36Z4euhfd1A0o8kdY3NZV+jlHh7uwr1em8TCFJSPyt9RNJc59y9VUydK+mgtOtOk3SDpOOdcxtic9tLqrL7Faglof/STxfZ1yknSfq7c+7zWJ19jVIS2tuZ1Ou9zY8M/qmHpIslnZj2z61OD8x7TlKvtPFwSc0kTU9d81Da106QNKXGVgwkYGY7SzpZUqanXpK/r6XMPQfsa9QJZjZO0huSOpjZ52Z2aWBaZG+b2Tlm9rmkYyRNMbNpaXPr9d7mnx3mwMwmSboh1mgVn9NI0iuSesY7t4G6iH2NcsXeToZAkAMz6yBp79Q/eck0p52k1s65l2ttYUAe2NcoV+ztZAgEAACAHgIAAEAgAAAAIhAAAAARCAAAgAgEAABABAIAACDp/wDs+ye8LGOXfgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = tfds.show_examples(mnist_ds_train, mnist_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c8c22-f95b-4f99-b553-a3ee58ddb02b",
   "metadata": {},
   "source": [
    "# Exaple: Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ef1b4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fmnist_train, fmnist_test), fminst_info = tfds.load(\n",
    "  name=\"fashion_mnist\",\n",
    "  as_supervised=True,\n",
    "  with_info=True,\n",
    "  split=[\"train\", \"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "41b0e433-cf3c-4222-95ad-93b0829b3e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturesDict({\n",
      "    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "})\n",
      "Class names: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "print(fminst_info.features)\n",
    "\n",
    "print(\"Class names:\", fashion_info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e16522bc-289f-468a-83b9-66d124bcb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Rescaling, Input, Reshape, Resizing\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0d801314-521a-421a-98af-17b066a2d48a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mragged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtype_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "`Input()` is used to instantiate a Keras tensor.\n",
       "\n",
       "A Keras tensor is a symbolic tensor-like object,\n",
       "which we augment with certain attributes that allow us to build a Keras model\n",
       "just by knowing the inputs and outputs of the model.\n",
       "\n",
       "For instance, if `a`, `b` and `c` are Keras tensors,\n",
       "it becomes possible to do:\n",
       "`model = Model(input=[a, b], output=c)`\n",
       "\n",
       "Args:\n",
       "    shape: A shape tuple (integers), not including the batch size.\n",
       "        For instance, `shape=(32,)` indicates that the expected input\n",
       "        will be batches of 32-dimensional vectors. Elements of this tuple\n",
       "        can be None; 'None' elements represent dimensions where the shape is\n",
       "        not known.\n",
       "    batch_size: optional static batch size (integer).\n",
       "    name: An optional name string for the layer.\n",
       "        Should be unique in a model (do not reuse the same name twice).\n",
       "        It will be autogenerated if it isn't provided.\n",
       "    dtype: The data type expected by the input, as a string\n",
       "        (`float32`, `float64`, `int32`...)\n",
       "    sparse: A boolean specifying whether the placeholder to be created is\n",
       "        sparse. Only one of 'ragged' and 'sparse' can be True. Note that,\n",
       "        if `sparse` is False, sparse tensors can still be passed into the\n",
       "        input - they will be densified with a default value of 0.\n",
       "    tensor: Optional existing tensor to wrap into the `Input` layer.\n",
       "        If set, the layer will use the `tf.TypeSpec` of this tensor rather\n",
       "        than creating a new placeholder tensor.\n",
       "    ragged: A boolean specifying whether the placeholder to be created is\n",
       "        ragged. Only one of 'ragged' and 'sparse' can be True. In this case,\n",
       "        values of 'None' in the 'shape' argument represent ragged dimensions.\n",
       "        For more information about RaggedTensors, see\n",
       "        [this guide](https://www.tensorflow.org/guide/ragged_tensors).\n",
       "    type_spec: A `tf.TypeSpec` object to create the input placeholder from.\n",
       "        When provided, all other args except name must be None.\n",
       "    **kwargs: deprecated arguments support. Supports `batch_shape` and\n",
       "        `batch_input_shape`.\n",
       "\n",
       "Returns:\n",
       "  A `tensor`.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "# this is a logistic regression in Keras\n",
       "x = Input(shape=(32,))\n",
       "y = Dense(16, activation='softmax')(x)\n",
       "model = Model(x, y)\n",
       "```\n",
       "\n",
       "Note that even if eager execution is enabled,\n",
       "`Input` produces a symbolic tensor-like object (i.e. a placeholder).\n",
       "This symbolic tensor-like object can be used with lower-level\n",
       "TensorFlow ops that take tensors as inputs, as such:\n",
       "\n",
       "```python\n",
       "x = Input(shape=(32,))\n",
       "y = tf.square(x)  # This op will be treated like a layer\n",
       "model = Model(x, y)\n",
       "```\n",
       "\n",
       "(This behavior does not work for higher-order TensorFlow APIs such as\n",
       "control flow and being directly watched by a `tf.GradientTape`).\n",
       "\n",
       "However, the resulting model will not track any variables that were\n",
       "used as inputs to TensorFlow ops. All variable usages must happen within\n",
       "Keras layers to make sure they will be tracked by the model's weights.\n",
       "\n",
       "The Keras Input can also create a placeholder from an arbitrary `tf.TypeSpec`,\n",
       "e.g:\n",
       "\n",
       "```python\n",
       "x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None],\n",
       "                                        dtype=tf.float32, ragged_rank=1))\n",
       "y = x.values\n",
       "model = Model(x, y)\n",
       "```\n",
       "When passing an arbitrary `tf.TypeSpec`, it must represent the signature of an\n",
       "entire batch instead of just one example.\n",
       "\n",
       "Raises:\n",
       "  ValueError: If both `sparse` and `ragged` are provided.\n",
       "  ValueError: If both `shape` and (`batch_input_shape` or `batch_shape`) are\n",
       "    provided.\n",
       "  ValueError: If `shape`, `tensor` and `type_spec` are None.\n",
       "  ValueError: If arguments besides `type_spec` are non-None while `type_spec`\n",
       "              is passed.\n",
       "  ValueError: if any unrecognized parameters are provided.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/git/marco/learn-deep-learning/.env/lib/python3.8/site-packages/keras/engine/input_layer.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fbf4df0c-9991-4326-8f5e-5b6fb38f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "346a225a-7517-4795-933d-a7641cd282d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,018\n",
      "Trainable params: 53,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=fashion_info.features[\"image\"].shape),\n",
    "    Rescaling(scale=1./127.5, offset=-1),\n",
    "    Flatten(),\n",
    "    Dense(units=64, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "    # Dropout(0.2),\n",
    "    Dense(units=32, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "    # Dropout(0.2),\n",
    "    Dense(units=16, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "    # Dropout(0.2),\n",
    "    Dense(units=10, activation=\"softmax\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "556c25ef-07e9-4908-98f5-2b946980d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_train_batched = fmnist_train.batch(BATCH_SIZE).cache().prefetch(1)\n",
    "fmnist_test_batched = fmnist_test.batch(BATCH_SIZE).cache().prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9d783551-db2b-42ea-9bf4-31ef22ffc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.1545 - accuracy: 0.7859 - val_loss: 0.8437 - val_accuracy: 0.7956\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.7639 - accuracy: 0.8206 - val_loss: 0.7718 - val_accuracy: 0.8025\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.7174 - accuracy: 0.8249 - val_loss: 0.7344 - val_accuracy: 0.8084\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6897 - accuracy: 0.8282 - val_loss: 0.7153 - val_accuracy: 0.8119\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6685 - accuracy: 0.8310 - val_loss: 0.6990 - val_accuracy: 0.8143\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6515 - accuracy: 0.8346 - val_loss: 0.6882 - val_accuracy: 0.8167\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6379 - accuracy: 0.8364 - val_loss: 0.6748 - val_accuracy: 0.8189\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6269 - accuracy: 0.8381 - val_loss: 0.6647 - val_accuracy: 0.8197\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6179 - accuracy: 0.8390 - val_loss: 0.6560 - val_accuracy: 0.8212\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.6102 - accuracy: 0.8401 - val_loss: 0.6492 - val_accuracy: 0.8205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb979d63100>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    fmnist_train_batched,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=fmnist_test_batched,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f7525-0e01-4920-94a5-33c136cd1fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-deep-learning",
   "language": "python",
   "name": "learn-deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
